import gradio as gr
import requests
import logging
import time
import re
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# URL Ollama API
OLLAMA_API = "http://ollama:11434"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
def test_ollama_connection(retries=3, delay=3):
    for i in range(retries):
        try:
            response = requests.get(f"{OLLAMA_API}/api/tags", timeout=5)
            response.raise_for_status()
            logging.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama")
            return True
        except requests.exceptions.RequestException as e:
            logging.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ({i+1}/{retries}): {e}")
            time.sleep(delay)
    logging.error("üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return False

if not test_ollama_connection():
    exit(1)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ LLM
def get_llm_engine(model_name):
    logging.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_name}")
    try:
        return ChatOllama(model=model_name, base_url=OLLAMA_API, temperature=0.3)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
SYSTEM_TEMPLATE = """You are an expert AI coding assistant. Provide concise, correct solutions 
with strategic print statements for debugging. Always respond in English using Markdown formatting for better readability."""

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_TEMPLATE),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# –ö–ª–∞—Å—Å —á–∞—Ç-–±–æ—Ç–∞
class ChatBot:
    def __init__(self):
        self.chat_history = [
            # {"role": "assistant", "content": "Hi! I'm **DeepSeek**. How can I help you code today? üíª"}
        ]
    def clean_response(self, response):
        response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL).strip()
        return response
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    def generate_ai_response(self, user_input, llm_engine):
        logging.info(f"üìù –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ –º–æ–¥–µ–ª—å: {user_input}")
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã LangChain –Ω–µ –ª–æ–º–∞–ª—Å—è
        formatted_history = [
            message.content if isinstance(message, (HumanMessage, AIMessage)) else message
            for message in self.chat_history
        ]
        self.chat_history.append(HumanMessage(content=user_input))
        chain = chat_prompt | llm_engine | StrOutputParser()
        response = chain.invoke({"input": user_input, "chat_history": formatted_history}) or "‚ö†Ô∏è –û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—Ç–≤–µ—Ç."
        response = self.clean_response(response)  # –û—á–∏—â–∞–µ–º –æ—Ç <think>...</think>
        self.chat_history.append(AIMessage(content=response))
        logging.info(f"üí° –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏: {response}")
        return response
    # –û—Ç—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ Gradio
    def chat(self, message, model_choice, history):
        if not message:
            return "", history
        llm_engine = get_llm_engine(model_choice)
        ai_response = self.generate_ai_response(message, llm_engine)
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ai_response})
        logging.info(f"üìú –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞: {history}")
        return "", history
    # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Ç–∞
    def chat_test(self, message, model_choice, history):
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "This is a test response."})
        return "", history

        

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Gradio
def create_demo():
    chatbot = ChatBot()

    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue", neutral_hue="zinc")) as demo:
        gr.Markdown("# üß† DeepSeek Code Companion")
        gr.Markdown("üöÄ Your AI Pair Programmer with Debugging Superpowers")
            
        with gr.Row():
            with gr.Column(scale=4):
                chatbot_component = gr.Chatbot(
                    value=[{"role": "assistant", "content": "Hi! I'm **DeepSeek**. How can I help you code today? üíª"}],  
                    show_copy_button=True,
                    height=500,
                    type="messages",
                    render_markdown=True,
                )
                msg = gr.Textbox(placeholder="Type your coding question here...", show_label=False)

                # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
                clear = gr.ClearButton([msg, chatbot_component])

            with gr.Column(scale=1):
                model_dropdown = gr.Dropdown(
                    choices=["deepseek-r1:1.5b", "deepseek-r1:7b", "deepseek-r1:14b"],
                    value="deepseek-r1:1.5b",
                    label="Choose Model"
                )
                gr.Markdown("### Model Capabilities")
                gr.Markdown("""
                - üêç Python Expert
                - üêû Debugging Assistant
                - üìù Code Documentation
                - üí° Solution Design
                """)

                gr.Markdown("Built with [Ollama](https://ollama.ai/) | [LangChain](https://python.langchain.com/)")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏–π
        msg.submit(chatbot.chat, [msg, model_dropdown, chatbot_component], [msg, chatbot_component])

    return demo

if __name__ == "__main__":
    demo = create_demo()
    demo.launch(server_name="0.0.0.0", server_port=7860)
