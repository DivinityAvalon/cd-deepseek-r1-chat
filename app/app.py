import gradio as gr
import numpy as np
import requests
import logging
import re
import time
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)
from langchain_core.messages import HumanMessage, AIMessage

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# URL Ollama API
OLLAMA_API = "http://ollama:11434"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
def test_ollama_connection(retries=3, delay=3):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞ Ollama –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        retries (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ç–∫–∞–∑–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3).
        delay (int): –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3 —Å–µ–∫).

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: True, –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ; False, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫.
    """
    for i in range(retries):
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º GET-–∑–∞–ø—Ä–æ—Å –∫ API Ollama
            response = requests.get(f"{OLLAMA_API}/api/tags", timeout=5)

            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —É—Å–ø–µ—à–µ–Ω (—Å—Ç–∞—Ç—É—Å 200), –∑–Ω–∞—á–∏—Ç —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç
            response.raise_for_status()
            logging.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama")
            return True  # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ

        except requests.exceptions.RequestException as e:
            # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç), –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
            logging.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ({i+1}/{retries}): {e}")

            # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            time.sleep(delay)

    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å, –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
    logging.error("üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return False

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
if not test_ollama_connection():
    exit(1) 

def get_llm_engine(model_name, temperature, top_p, top_k, repetition_penalty):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç LLM-–º–æ–¥–µ–ª—å —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "deepseek-r1:1.5b").
        temperature (float): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        top_p (float): –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–º—É –ø–æ—Ä–æ–≥—É (Nucleus Sampling).
        top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (Top-K Sampling).
        repetition_penalty (float): –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        ChatOllama: –û–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞.
        None: –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.
    """
    try:
        return ChatOllama(
            model=model_name,
            base_url=OLLAMA_API,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,  # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        )
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
SYSTEM_TEMPLATE = """You are an expert AI coding assistant. Provide concise, correct solutions 
with strategic print statements for debugging. Always respond in English."""

chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(SYSTEM_TEMPLATE),
    MessagesPlaceholder(variable_name="chat_history"),
    HumanMessagePromptTemplate.from_template("{input}")
])

# –ö–ª–∞—Å—Å —á–∞—Ç-–±–æ—Ç–∞
class ChatBot:
    """
    –ö–ª–∞—Å—Å ChatBot —É–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏–µ–π —á–∞—Ç–∞ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —Å LLM-–º–æ–¥–µ–ª—å—é.
    –û–Ω –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª—å, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö.
    """

    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞. –ò—Å—Ç–æ—Ä–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.
        """
        self.chat_history = [
            {"role": "assistant", "content": "Hi! I'm **DeepSeek**. How can I help you code today? üíª"}
        ]

    def generate_ai_response(self, user_input, llm_engine):
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –º–æ–¥–µ–ª–∏, –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è (<think>...</think>).

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            user_input (str): –í—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            llm_engine (ChatOllama): –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è LLM-–º–æ–¥–µ–ª—å.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            tuple: (—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç).
        """
        logging.info(f"üìù –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ –º–æ–¥–µ–ª—å: {user_input}")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ HumanMessage
        self.chat_history.append(HumanMessage(content=user_input))

        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É –≤—ã–∑–æ–≤–∞ LangChain: –ø—Ä–æ–º–ø—Ç ‚Üí –º–æ–¥–µ–ª—å ‚Üí –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—Ç—Ä–æ–∫
        chain = chat_prompt | llm_engine | StrOutputParser()

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
        response = chain.invoke({"input": user_input, "chat_history": self.chat_history}) or "‚ö†Ô∏è –û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—Ç–≤–µ—Ç."

        # –†–∞–∑–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏, –≤—ã–¥–µ–ª—è—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è <think>...</think>
        thoughts_match = re.search(r"<think>(.*?)</think>", response, re.DOTALL)
        if thoughts_match:
            thoughts = thoughts_match.group(1).strip()  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –º–µ–∂–¥—É <think> –∏ </think>
            main_response = response.replace(thoughts_match.group(0), "").strip()  # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        else:
            thoughts = None  # –ï—Å–ª–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º None
            main_response = response.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞

        logging.info(f"üí° –†–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n{thoughts if thoughts else '–ù–µ—Ç —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π'}")
        logging.info(f"üí° –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç:\n{main_response}")

        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
        if thoughts:
            self.chat_history.append(AIMessage(content=f"ü§î **Model's Thoughts:**\n> {thoughts}"))

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
        self.chat_history.append(AIMessage(content=main_response))

        return thoughts, main_response  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç

    # –ß–∞—Ç –≤ Gradio
    def chat(self, message, model_choice, temperature, top_p, top_k, repetition_penalty, history):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ LLM-–º–æ–¥–µ–ª—å –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            message (str): –í—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            model_choice (str): –í—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.
            temperature (float): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.
            top_p (float): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ (nucleus sampling).
            top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.
            repetition_penalty (float): –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–ª–æ–≤.
            history (list): –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            tuple: ("", –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞)
        """
        if not message:
            return "", history

        # –ü–æ–ª—É—á–∞–µ–º LLM —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        llm_engine = get_llm_engine(model_choice, temperature, top_p, top_k, repetition_penalty)

        history.append({"role": "user", "content": message})

        thoughts, ai_response = self.generate_ai_response(message, llm_engine)

        if thoughts:
            history.append({"role": "assistant", "content": f"ü§î **Model's Thoughts:**\n> {thoughts}"})

        history.append({"role": "assistant", "content": ai_response})

        logging.info(f"üìú –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:\n{history}")

        return "", history
    
    # –¢–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–∞—Ç–∞ (–±–µ–∑ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–æ–¥–µ–ª–∏)
    def chat_test(self, message, model_choice, history):
        """
        –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á–∞—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–µ–∑ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–æ–¥–µ–ª–∏ LLM.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            message (str): –í—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            model_choice (str): –í—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —ç—Ç–æ–π —Ç–µ—Å—Ç–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏).
            history (list): –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user"/"assistant", "content": "—Ç–µ–∫—Å—Ç"}].

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            tuple: ("", –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞) ‚Äî –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –æ—á–∏—â–∞–µ—Ç –ø–æ–ª–µ –≤–≤–æ–¥–∞ –≤ Gradio.
        """
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
        history.append({"role": "user", "content": message})

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        history.append({"role": "assistant", "content": "This is a test response."})

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É (–¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–æ–ª—è –≤–≤–æ–¥–∞ –≤ Gradio) –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
        return "", history

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Gradio
def create_demo():
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio —Å –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
    """
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ ChatBot
    chatbot = ChatBot()

    # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio —Å —Ç–µ–º–æ–π Soft –∏ –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏
    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue", neutral_hue="zinc")) as demo:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
        gr.Markdown("# üß† DeepSeek Code Companion")  # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        gr.Markdown("üöÄ Your AI Pair Programmer with Debugging Superpowers")  # –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫
            
        with gr.Row():  # –†–∞–∑–º–µ—â–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Å—Ç—Ä–æ–∫–µ
            with gr.Column(scale=4):  # –õ–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ (–æ—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞)
                # –ß–∞—Ç–±–æ—Ç-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                chatbot_component = gr.Chatbot(
                    value=[{"role": "assistant", "content": "Hi! I'm **DeepSeek**. How can I help you code today? üíª"}],  
                    show_copy_button=True,  # –ü–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è
                    height=500,  # –í—ã—Å–æ—Ç–∞ —á–∞—Ç–∞
                    type="messages",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç OpenAI (role + content)
                    render_markdown=True,  # –ü–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å Markdown
                )

                # –ü–æ–ª–µ –≤–≤–æ–¥–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                msg = gr.Textbox(placeholder="Type your coding question here...", show_label=False)

                # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞ (–æ—á–∏—â–∞–µ—Ç –ø–æ–ª–µ –≤–≤–æ–¥–∞ –∏ —á–∞—Ç–±–æ—Ç-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç)
                clear = gr.ClearButton([msg, chatbot_component])

            with gr.Column(scale=1):  # –ü—Ä–∞–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ (–≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ)
                # –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ LLM
                model_dropdown = gr.Dropdown(
                    choices=["deepseek-r1:1.5b", "deepseek-r1:7b", "deepseek-r1:14b"],  # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
                    value="deepseek-r1:1.5b",  # –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    label="Choose Model"  # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è –≤—ã–±–æ—Ä–∞
                )

                # –ù–æ–≤—ã–π –≤—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
                temperature_dropdown = gr.Dropdown(
                    choices=[round(x, 1) for x in np.arange(0.1, 1.0, 0.1)],  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —à–∞–≥–∏ 0.1-0.7
                    value=0.3,  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    label="Temperature",
                )
                # –ù–æ–≤—ã–π —Å–ª–∞–π–¥–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ top_p (Nucleus Sampling)
                top_p_slider = gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    step=0.1,
                    value=1.0,
                    label="Top-P Sampling"
                )

                # –ù–æ–≤—ã–π —Å–ª–∞–π–¥–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ top_k (Top-K Sampling)
                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=100,
                    step=1,
                    value=50,
                    label="Top-K Sampling"
                )

                # –ù–æ–≤—ã–π —Å–ª–∞–π–¥–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ repetition_penalty
                repetition_penalty_slider = gr.Slider(
                    minimum=1.0,
                    maximum=2.0,
                    step=0.1,
                    value=1.0,
                    label="Repetition Penalty"
                )

                # –û–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏
                gr.Markdown("### Model Capabilities")  
                gr.Markdown("""
                - üêç **Python Expert**
                - üêû **Debugging Assistant**
                - üìù **Code Documentation**
                - üí° **Solution Design**
                """)

                # –°—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
                gr.Markdown("Built with [Ollama](https://ollama.ai/) | [LangChain](https://python.langchain.com/)")

        # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É —Å–æ–æ–±—â–µ–Ω–∏—è –∫ —Ñ—É–Ω–∫—Ü–∏–∏ chat() —É ChatBot
        msg.submit(chatbot.chat, [msg, model_dropdown, temperature_dropdown, top_p_slider, top_k_slider, repetition_penalty_slider, chatbot_component], [msg, chatbot_component])

    return demo  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—ä–µ–∫—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

if __name__ == "__main__":
    demo = create_demo()
    demo.launch(server_name="0.0.0.0", server_port=7860)