import gradio as gr
import requests
import logging
import threading
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# URL Ollama API
OLLAMA_API = "http://ollama:11434"

# –§–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
stop_flag = threading.Event()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
def test_ollama_connection():
    try:
        response = requests.get(f"{OLLAMA_API}/api/tags", timeout=5)
        response.raise_for_status()
        logging.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama")
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")

# –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
test_ollama_connection()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ LLM
def get_llm_engine(model_name):
    return ChatOllama(
        model=model_name,
        base_url=OLLAMA_API,
        temperature=0.3
    )

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
SYSTEM_TEMPLATE = """You are an expert AI coding assistant. Provide concise, correct solutions 
with strategic print statements for debugging. Always respond in English."""

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_TEMPLATE),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

class ChatBot:
    def __init__(self):
        self.chat_history = [
            {"role": "assistant", "content": "Hi! I'm DeepSeek. How can I help you code today? üíª"}
        ]

    def generate_ai_response(self, user_input, llm_engine):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI (–±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è —Ç–µ–≥–æ–≤)"""
        logging.info(f"üìù –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {user_input}")

        stop_flag.clear()

        # –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
        chain = chat_prompt | llm_engine | StrOutputParser()
        try:
            for chunk in chain.stream({"input": user_input, "chat_history": self.chat_history}):
                if stop_flag.is_set():
                    return "‚õî –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞."
                response += chunk
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            response = "‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

        logging.info(f"üí° –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏: {response}")

        return response  # –û—Å—Ç–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (—Å `<think>` –∏ –ø—Ä–æ—á–∏–º)

    def chat(self, message, model_choice, history):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞ –≤ Gradio"""
        if not message:
            return history, ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

        logging.debug(f"üì© –í—Ö–æ–¥—è—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {message}")
        logging.debug(f"üîÑ –í—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {model_choice}")

        llm_engine = get_llm_engine(model_choice)
        logging.debug("‚úÖ LLM-–¥–≤–∏–∂–æ–∫ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        history.append({"role": "user", "content": message})

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        ai_response = self.generate_ai_response(message, llm_engine)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç AI –≤ –∏—Å—Ç–æ—Ä–∏—é (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç)
        history.append({"role": "assistant", "content": ai_response})

        return history, ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –∏ –æ—á–∏—â–∞–µ–º –ø–æ–ª–µ –≤–≤–æ–¥–∞
    
    def stop_generation(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        stop_flag.set()

    def clear_chat(self):
        """–û—á–∏—Å—Ç–∫–∞ —á–∞—Ç–∞"""
        self.chat_history = []
        return []

def create_demo():
    chatbot = ChatBot()
    
    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue", neutral_hue="zinc")) as demo:
        gr.Markdown("# üß† DeepSeek Code Companion")
        gr.Markdown("üöÄ Your AI Pair Programmer with Debugging Superpowers")
        
        with gr.Row():
            with gr.Column(scale=4):
                chatbot_component = gr.Chatbot(
                    value=[
                        {"role": "assistant", "content": "Hi! I'm DeepSeek. How can I help you code today? üíª"}
                    ],
                    height=500,
                    type="messages"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —Ç–µ–∫—Å—Ç–∞
                )
                msg = gr.Textbox(
                    placeholder="Type your coding question here...",
                    show_label=False
                )
                with gr.Row():
                    stop_btn = gr.Button("‚õî –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å")
                    clear_btn = gr.Button("üóë –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç")
                
            with gr.Column(scale=1):
                model_dropdown = gr.Dropdown(
                    choices=["deepseek-r1:1.5b", "deepseek-r1:7b", "deepseek-r1:14b"],
                    value="deepseek-r1:1.5b",
                    label="Choose Model"
                )
                gr.Markdown("### Model Capabilities")
                gr.Markdown("""
                - üêç Python Expert
                - üêû Debugging Assistant
                - üìù Code Documentation
                - üí° Solution Design
                """)
                
                gr.Markdown("Built with [Ollama](https://ollama.ai/) | [LangChain](https://python.langchain.com/)")

        msg.submit(
            fn=chatbot.chat,
            inputs=[msg, model_dropdown, chatbot_component],
            outputs=[chatbot_component, msg]  # –û—á–∏—â–∞–µ–º –ø–æ–ª–µ –≤–≤–æ–¥–∞ –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏
        )
        stop_btn.click(fn=chatbot.stop_generation, inputs=[], outputs=[])
        clear_btn.click(fn=chatbot.clear_chat, inputs=[], outputs=[chatbot_component])

    return demo

if __name__ == "__main__":
    demo = create_demo()
    demo.launch(server_name="0.0.0.0", server_port=7860)